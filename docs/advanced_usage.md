# Advanced Usage

Once you're comfortable with the basics, you can start exploring the advanced features that make FastCCG a powerful tool for building modern, high-performance applications.

---

## 1. Asynchronous Operations with `ask_async()`

In modern applications, blocking operations can be a bottleneck. FastCCG is built with an async-first philosophy, and the `.ask_async()` method is the key to unlocking non-blocking performance.

Instead of waiting for a response, `ask_async()` returns an awaitable coroutine that you can run concurrently with other tasks. This is essential for building responsive applications, such as web servers or chatbots that need to handle multiple users at once.

Here's how you can use it:

```python
import asyncio
import fastccg
from fastccg.models.gpt import gpt_4o

api_key = fastccg.add_openai_key("sk-...")
model = fastccg.init_model(gpt_4o, api_key=api_key)

async def main():
    # You can run multiple prompts concurrently
    task1 = model.ask_async("What is the speed of light?")
    task2 = model.ask_async("What is the capital of Australia?")

    # Wait for both to complete
    responses = await asyncio.gather(task1, task2)

    for response in responses:
        print(response.content)

asyncio.run(main())
```

## 2. Streaming Responses with `ask_stream()`

For real-time applications like chatbots, waiting for the full response can feel slow. The `.ask_stream()` method solves this by returning an **asynchronous generator** that yields response chunks as soon as they are generated by the model.

This allows you to display the response to the user word by word, creating a much more interactive and engaging experience.

```python
import asyncio
import fastccg
from fastccg.models.gpt import gpt_4o

api_key = fastccg.add_openai_key("sk-...")
model = fastccg.init_model(gpt_4o, api_key=api_key)

async def main():
    print("AI: ", end="")
    async for chunk in model.ask_stream("Tell me a short story about a robot who learns to paint."):
        print(chunk.content, end="", flush=True)
    print()

asyncio.run(main())
```

## 3. Saving and Loading Sessions

FastCCG makes it easy to persist and restore your conversation history. This is incredibly useful for resuming a previous session or creating checkpoints in a long-running process.

### Saving a Session

To save the current state of the model, including the full conversation history and all configurations, use the `.save()` method.

```python
model.save("my_session.json")
```

This will create a `my_session.json` file with the complete state of the model. **For security, your API key is never saved.**

### Loading a Session

To restore a session, use the top-level `load_model()` function. You'll need to provide the path to the saved file and your API key.

```python
import fastccg

api_key = fastccg.add_openai_key("sk-...")

# Load the session from the file
loaded_model = fastccg.load_model("my_session.json", api_key=api_key)

# You can now continue the conversation
response = loaded_model.ask("Based on our previous conversation, what was my last question?")
print(response.content)
```

---

For a detailed breakdown of every available function, see the **[API Reference](./api_reference.md)**.
